{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P2B One vs ALL logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# IMPORTS #############################\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import brown\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import numpy as np\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# INITIALIZATIONS #############################\n",
    "data = []\n",
    "raw_data = brown.tagged_words()\n",
    "\n",
    "Vocab = set()\n",
    "tag_list = set()\n",
    "tag_num_dict = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# DATA PREPROCESSING #############################\n",
    "def hasNumbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)\n",
    "\n",
    "def pre_process(data):\n",
    "    data1 = []\n",
    "\n",
    "    for i in range(len(data[0:2000])):\n",
    "        if hasNumbers(data[i][0]):\n",
    "            continue\n",
    "        \n",
    "        elif data[i][0] == '.':\n",
    "            data1.append(('.','.'))\n",
    "            \n",
    "        elif len(data[i][1]) == 1:\n",
    "            continue\n",
    "        \n",
    "        elif data[i][0]==data[i][1]:\n",
    "            continue\n",
    "        else:\n",
    "            if data[i][1].find('-')!=-1:\n",
    "                temp = data[i][1].split(\"-\")\n",
    "                temp[0] = temp[0].replace(\"*\",\"\")\n",
    "                temp[0] = temp[0].replace(\"$\",\"\")\n",
    "                temp[0] = temp[0].replace(\")\",\"\")\n",
    "                temp[0] = temp[0].replace(\"(\",\"\")\n",
    "                temp[0] = temp[0].replace(\":\",\"\")\n",
    "                temp[0] = temp[0].replace(\",\",\"\")\n",
    "                temp[0] = temp[0].replace(\".\",\"\")\n",
    "                \n",
    "                if len(temp[0]) > 1:\n",
    "                    data1.append((data[i][0],temp[0][:2]))\n",
    "            \n",
    "            elif data[i][1].find('+')!=-1:\n",
    "                temp = data[i][1].split(\"+\")\n",
    "                temp[0] = temp[0].replace(\"*\",\"\")\n",
    "                temp[0] = temp[0].replace(\"$\",\"\")\n",
    "                temp[0] = temp[0].replace(\")\",\"\")\n",
    "                temp[0] = temp[0].replace(\"(\",\"\")\n",
    "                temp[0] = temp[0].replace(\":\",\"\")\n",
    "                temp[0] = temp[0].replace(\".\",\"\")\n",
    "                temp[0] = temp[0].replace(\",\",\"\")\n",
    "                \n",
    "                if len(temp[0]) > 1:\n",
    "                    data1.append((data[i][0],temp[0][:2]))   \n",
    "            else:\n",
    "                temp = data[i][1].replace(\"*\",\"\")\n",
    "                temp = temp.replace(\"*\",\"\")\n",
    "                temp = temp.replace(\"$\",\"\")\n",
    "                temp = temp.replace(\")\",\"\")\n",
    "                temp = temp.replace(\"(\",\"\")\n",
    "                temp = temp.replace(\":\",\"\")\n",
    "                temp = temp.replace(\".\",\"\")\n",
    "                temp = temp.replace(\",\",\"\")\n",
    "                \n",
    "                if len(temp) > 1:\n",
    "                    data1.append((data[i][0],temp[:2]))\n",
    "    data1.pop()\n",
    "    data1.pop()\n",
    "    processed_data = []\n",
    "    temp = []\n",
    "    \n",
    "    for i in range(len(data1)):\n",
    "        if data1[i][0] == '.':\n",
    "            temp.append(tuple(data1[i]))\n",
    "            processed_data.append(temp)\n",
    "            temp = []\n",
    "        else:\n",
    "            temp.append(data1[i])\n",
    "    \n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Populate dictionaries ##############\n",
    "\n",
    "def build_dicts(sentences):\n",
    "    global Vocab\n",
    "    global tag_list , tag_num_dict\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        \n",
    "        for words in sentence:\n",
    "            Vocab.add(words[0])\n",
    "            tag_list.add(words[1])\n",
    "    index=0  \n",
    "    for i in tag_list:\n",
    "        tag_num_dict[i]=index\n",
    "        index+=1\n",
    "    tag_num_dict[\"UK\"] = index    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### FEATURE EXTRACTION #########################\n",
    "def get_feature(token, token_index, sent):\n",
    "    #global token_feature\n",
    "    token_feature = { 'token'             : token,\n",
    "                    'is_first'          : token_index == 0,\n",
    "                    'is_last'           : token_index == len(sent)-1,\n",
    "\n",
    "                    'is_capitalized'    : token[0].upper() == token[0],\n",
    "                    'is_all_capitalized': token.upper() == token,\n",
    "                    'is_capitals_inside': token[1:].lower() != token[1:],\n",
    "                    'is_numeric'        : token.isdigit(),\n",
    "\n",
    "                    'prefix-1'          : token[0],\n",
    "                    'prefix-2'          : '' if len(token) < 2  else token[:1],\n",
    "\n",
    "                    'suffix-1'          : token[-1],\n",
    "                    'suffix-2'          : '' if len(token) < 2  else token[-2:],\n",
    "\n",
    "                    'prev-token'        : '' if token_index == 0     else sent[token_index - 1][0],\n",
    "                    '2-prev-token'      : '' if token_index <= 1     else sent[token_index - 2][0],\n",
    "\n",
    "                    'next-token'        : '' if token_index == len(sent) - 1     else sent[token_index + 1][0],\n",
    "                    '2-next-token'      : '' if token_index >= len(sent) - 2     else sent[token_index + 2][0]\n",
    "                    }\n",
    "\n",
    "    return  token_feature\n",
    "\n",
    "def feature_extraction(pdata):\n",
    "    global tag_num_dict\n",
    "    features = []\n",
    "    pos_labels = []\n",
    "    \n",
    "    for sent in pdata:\n",
    "        for token_index, token_pair in enumerate(sent):\n",
    "            features.append(get_feature(token_pair[0],token_index,sent))\n",
    "            pos_labels.append(tag_num_dict.get(token_pair[1],-1))\n",
    "    \n",
    "    return features,pos_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# ONE vs ALL LOGISTIC REGRESSION #############################\n",
    "def costFunction(features, target, weights):\n",
    "    cost = np.array([],dtype=np.float128)\n",
    "    scores = np.array([],dtype=np.float128)\n",
    "    scores = np.dot(features, weights)\n",
    "    hypothesis = sigmoid(scores)\n",
    "    cost = np.sum((-target*np.log(hypothesis))-((1-target)*np.log(1-hypothesis)))   \n",
    "    return cost\n",
    "\n",
    "def sigmoid(scores):\n",
    "    return 1/(1 + np.exp(-scores))\n",
    "\n",
    "def logistic_Regression(wordFeature,targetPOS,num_epochs,learning_rate):\n",
    "    targetPOS = np.array(targetPOS)\n",
    "    num_classes = len(tag_list)\n",
    "    num_feature = wordFeature.shape[1]\n",
    "    classifier = np.zeros(shape=(num_classes+1,num_feature),dtype=np.float128)\n",
    "    \n",
    "    print(\"Training for label           Cost Function\\n\")\n",
    "    for c in range(0,num_classes+1):\n",
    "        #print(\"Training for label: \",c)\n",
    "        targetLabel =  (targetPOS==c).astype(int)\n",
    "        weights = np.zeros(wordFeature.shape[1],dtype=np.float128)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            scores = np.dot(wordFeature,weights)\n",
    "            predictions = sigmoid(scores)\n",
    "            \n",
    "            output_error = targetLabel - predictions\n",
    "            gradient = np.dot(wordFeature.T,output_error)\n",
    "            weights += (learning_rate * gradient)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                cost = costFunction(wordFeature, targetLabel, weights)\n",
    "#                 print(\"costFunction: \",cost)\n",
    "                print(c,\"                          \",cost)\n",
    "        classifier[c,:] = weights\n",
    "\n",
    "    return classifier\n",
    "\n",
    "def calculate_accuracy(predictions,actualPOS):\n",
    "    correct = 0\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] == actualPOS[i]:\n",
    "            correct += 1\n",
    "    \n",
    "    correct /= len(predictions)\n",
    "    print(\"Accuracy :\",correct*100 , \"%\")\n",
    "\n",
    "def evaluate(weights,testdata,actualPOS):\n",
    "    actualPOS = np.array(actualPOS)\n",
    "\n",
    "    scores = np.dot(testdata, weights.T)\n",
    "    scores = np.round(sigmoid(scores))\n",
    "    predictions = scores.argmax(axis=1)\t\n",
    "    calculate_accuracy(predictions,actualPOS)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## PREDICTING TAGS ########################\n",
    "def preprocess_sent(test_sent):\n",
    "    word_feature = []\n",
    "    sentence = test_sent.split(\" \")\n",
    "    \n",
    "    for token_index, token_pair in enumerate(sentence):\n",
    "        word_feature.append(get_feature(token_pair,token_index,test_sent))\n",
    "    \n",
    "    return sentence,word_feature\n",
    "\n",
    "def predict_POS(features,weights):\n",
    "    scores = np.dot(features,weights.T)\n",
    "    scores = np.round(sigmoid(scores))\n",
    "    predictions = scores.argmax(axis=1)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def pred_tag_sequence(test_sent,weights):\n",
    "    global tag_num_dict\n",
    "    estimatedPOS = []\n",
    "    num_tag_dict = {}\n",
    "    \n",
    "    sentence,word_feature = preprocess_sent(test_sent)\n",
    "    vectorized_features = v.transform(word_feature)\n",
    "    predictions = predict_POS(vectorized_features,weights)\n",
    "    \n",
    "    for key,val in tag_num_dict.items():\n",
    "        num_tag_dict[val] = key\n",
    "        \n",
    "    for index,sent in enumerate(sentence):\n",
    "        estimatedPOS.append((sent,num_tag_dict.get(predictions[index],len(num_tag_dict)-1)))\n",
    "        \n",
    "    print(\"Predicted POS Tags: \",estimatedPOS)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624\n",
      "29\n",
      "{'TO', 'RP', 'CS', 'NN', 'PP', 'AP', 'WR', 'AT', 'PN', 'RB', 'DO', 'QL', 'AB', 'NR', 'HV', 'OD', 'WD', 'MD', 'IN', 'CD', 'CC', 'EX', 'NP', '.', 'VB', 'DT', 'WP', 'JJ', 'BE'}\n"
     ]
    }
   ],
   "source": [
    "############################# MAIN #############################\n",
    "for i in raw_data:\n",
    "    data.append(i)\n",
    "processed_data = pre_process(data)\n",
    "\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "for i in range(len(processed_data)):\n",
    "    if i <= 0.8*len(processed_data):\n",
    "        train_data.append(processed_data[i])\n",
    "    else:\n",
    "        test_data.append(processed_data[i])\n",
    "\n",
    "build_dicts(train_data)\n",
    "features,pos_labels = feature_extraction(train_data)\n",
    "\n",
    "\n",
    "print(len(Vocab))\n",
    "print(len(tag_list))\n",
    "print(tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for label           Cost Function\n",
      "\n",
      "0                            396.93594038934859178\n",
      "1                            29.875715612838742474\n",
      "2                            644.3700331106500838\n",
      "3                            2127.2854119779570032\n",
      "4                            683.3259904137802483\n",
      "5                            117.03471020763982717\n",
      "6                            146.4587713416905014\n",
      "7                            517.4728822088071694\n",
      "8                            19.531349732243472409\n",
      "9                            417.9639697065304384\n",
      "10                            30.360584257222641142\n",
      "11                            43.56896223386971269\n",
      "12                            42.108836581550017804\n",
      "13                            201.6092389526472931\n",
      "14                            169.31884620043930342\n",
      "15                            37.560043922638565558\n",
      "16                            75.91352871063359986\n",
      "17                            251.31008106512473668\n",
      "18                            944.257092006873579\n",
      "19                            341.06098015869806922\n",
      "20                            430.87905950630769542\n",
      "21                            150.4123710798973273\n",
      "22                            1252.0535149433384189\n",
      "23                            0.2905470218683305911\n",
      "24                            1359.6951000594609047\n",
      "25                            536.61149116082211924\n",
      "26                            91.41162632752851443\n",
      "27                            1039.0163018665200436\n",
      "28                            675.7689590989180599\n",
      "29                            0.25773166683559488787\n"
     ]
    }
   ],
   "source": [
    "############################# TRAINING #############################\n",
    "v = DictVectorizer(sparse=False)\n",
    "train_x = v.fit_transform(features)\n",
    "weights = logistic_Regression(train_x,pos_labels,5,0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 62.22222222222222 %\n"
     ]
    }
   ],
   "source": [
    "############################# TESTING #############################\n",
    "features1,pos_labels1 = feature_extraction(test_data)\n",
    "test_x = v.transform(features1)\n",
    "evaluate(weights,test_x,pos_labels1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# PREDICTING TAG SEQUENCE #############################\n",
    "test_sent = input(\"Enter Sentence: \")\n",
    "pred_tag_sequence(test_sent,weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
